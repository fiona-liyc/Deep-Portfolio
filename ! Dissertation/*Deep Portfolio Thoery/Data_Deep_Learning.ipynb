{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the required packages\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as md\n",
    "import matplotlib.pylab as pylab\n",
    "import quandl\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas_datareader.data as web\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "matplotlib.rcParams[ 'figure.figsize' ] = ( 18, 7 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to retrieve data\n",
    "def GetData(fileName):\n",
    "    return pd.read_csv(fileName, header=0,usecols=['Date','Adj Close'], parse_dates=True, index_col='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'data_ivv' (DataFrame)\n",
      "Stored 'data_lqd' (DataFrame)\n",
      "Stored 'data_shy' (DataFrame)\n",
      "Stored 'data_tip' (DataFrame)\n",
      "Stored 'data_vig' (DataFrame)\n",
      "Stored 'data_vnq' (DataFrame)\n",
      "Stored 'data_gld' (DataFrame)\n",
      "Stored 'data_efa' (DataFrame)\n",
      "Stored 'data_eem' (DataFrame)\n",
      "Stored 'data_emb' (DataFrame)\n",
      "Stored 'data_mub' (DataFrame)\n",
      "Stored 'data_xle' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "data_ivv = GetData('Asset_Dataset/IVV.csv')\n",
    "%store data_ivv\n",
    "\n",
    "data_lqd = GetData('Asset_Dataset/LQD.csv')\n",
    "%store data_lqd\n",
    "\n",
    "data_shy = GetData('Asset_Dataset/SHY.csv')\n",
    "%store data_shy\n",
    "\n",
    "data_tip = GetData('Asset_Dataset/TIP.csv')\n",
    "%store data_tip\n",
    "\n",
    "data_vig = GetData('Asset_Dataset/VIG.csv')\n",
    "%store data_vig\n",
    "\n",
    "data_vnq = GetData('Asset_Dataset/VNQ.csv')\n",
    "%store data_vnq\n",
    "\n",
    "data_gld = GetData('Asset_Dataset/GLD.csv')\n",
    "%store data_gld\n",
    "\n",
    "data_efa = GetData('Asset_Dataset/EFA.csv')\n",
    "%store data_efa\n",
    "\n",
    "data_eem = GetData('Asset_Dataset/EEM.csv')\n",
    "%store data_eem\n",
    "\n",
    "data_emb = GetData('Asset_Dataset/EMB.csv')\n",
    "%store data_emb\n",
    "\n",
    "data_mub = GetData('Asset_Dataset/MUB.csv')\n",
    "%store data_mub\n",
    "\n",
    "data_xle = GetData('Asset_Dataset/XLE.csv')\n",
    "%store data_xle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Importing functions###\n",
    "\n",
    "def get_data(path, daterule):\n",
    "    '''\n",
    "    Format a Data frame with Adj Close prices for all tickers (columns) during the concerned date range (index).\n",
    "    Daterule item filters the number of dates we need\n",
    "    '''\n",
    "    list_dataframes = []\n",
    "    allFiles = glob.glob(path + \"/csv\" + \"*.csv\")\n",
    "    i = 0\n",
    "    list_tickers = []\n",
    "    \n",
    "    if computer == 0:\n",
    "        for file_ in allFiles:\n",
    "            with open(file_, 'r') as csvfile:\n",
    "                import_df = pd.read_csv(csvfile, index_col=None, header=0)\n",
    "                adj_close_arr = import_df['Adj Close'].values\n",
    "                ticker = file_.split('_')[3]\n",
    "                \n",
    "                if len(adj_close_arr) != daterule or np.isnan(adj_close_arr).any() or ticker == 'IBB' or ticker == '^GSPC':\n",
    "                    # If we don't have the specified number of obs, or some nan: ignore\n",
    "                    # Also ignore the IBB here. (Imported in Calibration)\n",
    "                    continue\n",
    "                    \n",
    "                list_dataframes = list_dataframes + list(adj_close_arr)\n",
    "                dates = import_df['Date'].values\n",
    "                list_tickers.append(ticker)\n",
    "                i += 1\n",
    "\n",
    "    if computer == 1:\n",
    "        for file_ in allFiles:\n",
    "            with open(file_, 'r', encoding ='mac_roman') as csvfile:\n",
    "                import_df = pd.read_csv(csvfile, index_col=None, header=0)\n",
    "                adj_close_arr = import_df['Adj Close'].values\n",
    "                ticker = file_.split('_')[3]\n",
    "                \n",
    "                if len(adj_close_arr) != daterule or np.isnan(adj_close_arr).any() or ticker == 'IBB'or ticker == '^GSPC':\n",
    "                    # If we don't have the specified number of obs, or some nan: ignore\n",
    "                    # Also ignore the IBB here. (Imported in Calibration)\n",
    "                    continue\n",
    "                    \n",
    "                list_dataframes = list_dataframes + list(adj_close_arr)\n",
    "                dates = import_df['Date'].values\n",
    "                list_tickers.append(ticker)\n",
    "                i += 1\n",
    "    \n",
    "    num_files = i\n",
    "    # Reshape our list to an array with the good shape, ready to be fitted in a dataframe\n",
    "    list_dataframes = np.reshape(list_dataframes, [len(list_tickers), len(dates)]).T\n",
    "    df_out_adj_close = pd.DataFrame(list_dataframes, index=dates, columns=list_tickers)\n",
    "    \n",
    "    return num_files, df_out_adj_close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(df):\n",
    "    '''\n",
    "    Use the preprocessing object MinMaxScaler from sklearn library to normalize the data.\n",
    "    The out dataframe has the same index and column\n",
    "    '''\n",
    "    x = df.values \n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    x_scaled = min_max_scaler.fit_transform(x)\n",
    "    df_n = pd.DataFrame(x_scaled, index=df.index, columns=df.columns)\n",
    "    return df_n\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
