{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import the required packages\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as md\n",
    "import matplotlib.pylab as pylab\n",
    "import quandl\n",
    "import pandas as pd\n",
    "import copy\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas_datareader.data as web\n",
    "import datetime\n",
    "from time import time\n",
    "\n",
    "#Deep learn\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics\n",
    "import statsmodels.api as sm\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model, load_model\n",
    "from keras import regularizers\n",
    "from collections import defaultdict\n",
    "\n",
    "#layout\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams[ 'figure.figsize' ] = ( 18, 7 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to retrieve data\n",
    "def GetData(fileName):\n",
    "    return pd.read_csv(fileName, header=0,usecols=['Date','Adj Close'], parse_dates=True, index_col='Date').astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'data_ivv' (DataFrame)\n",
      "Stored 'data_lqd' (DataFrame)\n",
      "Stored 'data_shy' (DataFrame)\n",
      "Stored 'data_vnq' (DataFrame)\n",
      "Stored 'data_gld' (DataFrame)\n",
      "Stored 'data_efa' (DataFrame)\n",
      "Stored 'data_eem' (DataFrame)\n",
      "Stored 'data_emb' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "##should i add the other assets back\n",
    "#???\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "data_ivv = GetData('Asset_Dataset/IVV.csv')\n",
    "%store data_ivv\n",
    "\n",
    "data_lqd = GetData('Asset_Dataset/LQD.csv')\n",
    "%store data_lqd\n",
    "\n",
    "data_shy = GetData('Asset_Dataset/SHY.csv')\n",
    "%store data_shy\n",
    "\n",
    "data_vnq = GetData('Asset_Dataset/VNQ.csv')\n",
    "%store data_vnq\n",
    "\n",
    "data_gld = GetData('Asset_Dataset/GLD.csv')\n",
    "%store data_gld\n",
    "\n",
    "data_efa = GetData('Asset_Dataset/EFA.csv')\n",
    "%store data_efa\n",
    "\n",
    "data_eem = GetData('Asset_Dataset/EEM.csv')\n",
    "%store data_eem\n",
    "\n",
    "data_emb = GetData('Asset_Dataset/EMB.csv')\n",
    "%store data_emb\n",
    "\n",
    "asset_table = pd.concat([data_ivv,data_lqd,data_shy,data_vnq,data_gld,data_efa,data_eem,data_emb],axis=1)\n",
    "asset_table.columns = ['IVV close','LQD close','SHY close','VNQ close','GLD close','EFA close','EEM close','EMB close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net change\n",
    "#def net_change(data):\n",
    "\n",
    "data_ivv_net = data_ivv.diff().dropna()\n",
    "data_lqd_net = data_lqd.diff().dropna()\n",
    "data_shy_net = data_shy.diff().dropna()\n",
    "data_vnq_net = data_vnq.diff().dropna()\n",
    "data_gld_net = data_gld.diff().dropna()\n",
    "data_efa_net = data_efa.diff().dropna()\n",
    "data_eem_net = data_eem.diff().dropna()\n",
    "data_emb_net = data_emb.diff().dropna()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Auto Encoding ###\n",
    "    #1)Train an auto encoder model which compresses market information\n",
    "    #2)Rank stocks by 2-normal difference -> select the subset as the portfolio\n",
    "    #3)Linear + RELU as activation functions (p7 of Heaton et al)\n",
    "    #4)W = (W1, W2), L2 regularization\n",
    "\n",
    "asset = defaultdict(defaultdict)    \n",
    "\n",
    "#5 neurons\n",
    "encoding_dimension = 5\n",
    "#number of features\n",
    "num_asset = len(asset_table)\n",
    "\n",
    "#connect all layers\n",
    "# see 'Stacked Auto-Encoders' in paper\n",
    "input_layer = Input(shape=(num_asset, ))\n",
    "encoded = Dense(encoding_dimension, activation='relu', kernel_regularizer=regularizers.l2(0.01))(input_layer)\n",
    "decoded = Dense(num_asset, activation= 'linear', kernel_regularizer=regularizers.l2(0.01))(encoded)\n",
    "    \n",
    "# construct and compile AE model\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "autoencoder.compile(optimizer='sgd', loss='mean_squared_error')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_6 to have shape (262,) but got array with shape (8,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-1bc37c546da6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train autoencoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#data_t = asset['calibrate']['net']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masset_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masset_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model/retrack_autoencoder.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_6 to have shape (262,) but got array with shape (8,)"
     ]
    }
   ],
   "source": [
    "# train autoencoder\n",
    "#data_t = asset['calibrate']['net']\n",
    "autoencoder.fit(asset_table, asset_table, shuffle=False, epochs=500, batch_size = 10)\n",
    "autoencoder.save('model/retrack_autoencoder.csv')\n",
    "\n",
    "# test/reconstruct market information matrix\n",
    "reconstruct = autoencoder.predict(data_t)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
